{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(arr):\n",
    "    array = []\n",
    "    for i,item in enumerate(arr):\n",
    "        if item > 0.5:\n",
    "            array.append(True)\n",
    "        else:\n",
    "            array.append(False)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"full_training_data_1988-2016\")\n",
    "test_data = pd.read_csv(\"testing_data_2017-2020\")\n",
    "\n",
    "x_train = train_data.iloc[:,1:-1].select_dtypes(exclude=['object'])\n",
    "y_train = train_data[\"All_NBA\"]\n",
    "x_test = test_data.iloc[:,1:-1].select_dtypes(exclude=['object'])\n",
    "y_test = test_data[\"All_NBA\"]\n",
    "\n",
    "x_train_numpy = x_train.to_numpy()\n",
    "y_train_numpy = y_train.to_numpy()\n",
    "\n",
    "x_test_numpy = x_test.to_numpy()\n",
    "y_test_numpy = y_test.to_numpy()\n",
    "\n",
    "print(x_train.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_6 (Dense)              (None, 90)                8190      \n_________________________________________________________________\ndense_7 (Dense)              (None, 50)                4550      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 32)                1632      \n_________________________________________________________________\ndense_9 (Dense)              (None, 16)                528       \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 8)                 136       \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 8)                 0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 4)                 36        \n_________________________________________________________________\ndense_12 (Dense)             (None, 1)                 5         \n=================================================================\nTotal params: 15,077\nTrainable params: 15,077\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "neural_model = Sequential([\n",
    "     Dense(units=90, input_dim=90, activation='relu'),\n",
    "     Dense(units=50,activation='relu'),\n",
    "     Dropout(0.2),\n",
    "     Dense(32,activation='relu'),\n",
    "     Dense(16,activation='relu'),\n",
    "     Dropout(0.2),\n",
    "     Dense(8,activation='relu'),\n",
    "     Dropout(0.2),\n",
    "     Dense(4,activation='relu'),\n",
    "     Dense(1,activation='sigmoid')  \n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "neural_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4203e-04 - val_fn: 2014.8462 - val_fp: 1929.0000 - val_tn: 56426.4609 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0326\n",
      "Epoch 6/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 4.9029e-04 - fn: 2225.5872 - fp: 1929.0000 - tn: 62317.3672 - tp: 68.0000 - precision: 0.0341 - recall: 0.0297 - val_loss: 4.4541e-04 - val_fn: 2434.8462 - val_fp: 1929.0000 - val_tn: 68210.4609 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0272\n",
      "Epoch 7/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 4.0465e-04 - fn: 2643.4448 - fp: 1929.0000 - tn: 74103.4844 - tp: 68.0000 - precision: 0.0341 - recall: 0.0251 - val_loss: 3.6909e-04 - val_fn: 2854.8462 - val_fp: 1929.0000 - val_tn: 79994.4609 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0233\n",
      "Epoch 8/150\n",
      "10983/10983 [==============================] - 1s 55us/step - loss: 3.3674e-04 - fn: 3061.6221 - fp: 1929.0000 - tn: 85889.3203 - tp: 68.0000 - precision: 0.0341 - recall: 0.0218 - val_loss: 3.0819e-04 - val_fn: 3274.8462 - val_fp: 1929.0000 - val_tn: 91778.4609 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0203\n",
      "Epoch 9/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.8241e-04 - fn: 3487.6423 - fp: 1929.0000 - tn: 97667.3047 - tp: 68.0000 - precision: 0.0341 - recall: 0.0191 - val_loss: 2.5913e-04 - val_fn: 3694.8462 - val_fp: 1929.0000 - val_tn: 103562.4609 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0181\n",
      "Epoch 10/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.3854e-04 - fn: 3894.4272 - fp: 1929.0000 - tn: 109464.6016 - tp: 68.0000 - precision: 0.0341 - recall: 0.0172 - val_loss: 2.1927e-04 - val_fn: 4114.8462 - val_fp: 1929.0000 - val_tn: 115346.4609 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0163\n",
      "Epoch 11/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.0259e-04 - fn: 4322.0552 - fp: 1929.0000 - tn: 121240.9531 - tp: 68.0000 - precision: 0.0341 - recall: 0.0155 - val_loss: 1.8665e-04 - val_fn: 4534.8462 - val_fp: 1929.0000 - val_tn: 127130.4609 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0148\n",
      "Epoch 12/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 1.7316e-04 - fn: 4744.2793 - fp: 1929.0000 - tn: 133022.7188 - tp: 68.0000 - precision: 0.0341 - recall: 0.0141 - val_loss: 1.5977e-04 - val_fn: 4954.8462 - val_fp: 1929.0000 - val_tn: 138914.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0135\n",
      "Epoch 13/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 1.4884e-04 - fn: 5167.1484 - fp: 1929.0000 - tn: 144803.8906 - tp: 68.0000 - precision: 0.0341 - recall: 0.0130 - val_loss: 1.3746e-04 - val_fn: 5374.8462 - val_fp: 1929.0000 - val_tn: 150698.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0125\n",
      "Epoch 14/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 1.2863e-04 - fn: 5576.5264 - fp: 1929.0000 - tn: 156598.4375 - tp: 68.0000 - precision: 0.0341 - recall: 0.0121 - val_loss: 1.1885e-04 - val_fn: 5794.8462 - val_fp: 1929.0000 - val_tn: 162482.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0116\n",
      "Epoch 15/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 1.1176e-04 - fn: 6013.9302 - fp: 1929.0000 - tn: 168365.0469 - tp: 68.0000 - precision: 0.0341 - recall: 0.0112 - val_loss: 1.0328e-04 - val_fn: 6214.8462 - val_fp: 1929.0000 - val_tn: 174266.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0108\n",
      "Epoch 16/150\n",
      "10983/10983 [==============================] - 1s 55us/step - loss: 9.7591e-05 - fn: 6428.0640 - fp: 1929.0000 - tn: 180154.9375 - tp: 68.0000 - precision: 0.0341 - recall: 0.0105 - val_loss: 9.0166e-05 - val_fn: 6634.8462 - val_fp: 1929.0000 - val_tn: 186050.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0101\n",
      "Epoch 17/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 8.5688e-05 - fn: 6838.6484 - fp: 1929.0000 - tn: 191948.3906 - tp: 68.0000 - precision: 0.0341 - recall: 0.0098 - val_loss: 7.9109e-05 - val_fn: 7054.8462 - val_fp: 1929.0000 - val_tn: 197834.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0095\n",
      "Epoch 18/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 7.5627e-05 - fn: 7270.4536 - fp: 1929.0000 - tn: 203720.4844 - tp: 68.0000 - precision: 0.0341 - recall: 0.0093 - val_loss: 6.9763e-05 - val_fn: 7474.8462 - val_fp: 1929.0000 - val_tn: 209618.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0090\n",
      "Epoch 19/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 6.7144e-05 - fn: 7677.2412 - fp: 1929.0000 - tn: 215517.7188 - tp: 68.0000 - precision: 0.0341 - recall: 0.0088 - val_loss: 6.1855e-05 - val_fn: 7894.8462 - val_fp: 1929.0000 - val_tn: 221402.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0085\n",
      "Epoch 20/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 5.9949e-05 - fn: 8092.8169 - fp: 1929.0000 - tn: 227306.2031 - tp: 68.0000 - precision: 0.0341 - recall: 0.0083 - val_loss: 5.5143e-05 - val_fn: 8314.8457 - val_fp: 1929.0000 - val_tn: 233186.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0081\n",
      "Epoch 21/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 5.3856e-05 - fn: 8527.8223 - fp: 1929.0000 - tn: 239075.2031 - tp: 68.0000 - precision: 0.0341 - recall: 0.0079 - val_loss: 4.9435e-05 - val_fn: 8734.8457 - val_fp: 1929.0000 - val_tn: 244970.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0077\n",
      "Epoch 22/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 4.8683e-05 - fn: 8950.0469 - fp: 1929.0000 - tn: 250856.9062 - tp: 68.0000 - precision: 0.0341 - recall: 0.0075 - val_loss: 4.4591e-05 - val_fn: 9154.8457 - val_fp: 1929.0000 - val_tn: 256754.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0074\n",
      "Epoch 23/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 4.4303e-05 - fn: 9371.9561 - fp: 1929.0000 - tn: 262639.1250 - tp: 68.0000 - precision: 0.0341 - recall: 0.0072 - val_loss: 4.0477e-05 - val_fn: 9574.8457 - val_fp: 1929.0000 - val_tn: 268538.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0071\n",
      "Epoch 24/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 4.0595e-05 - fn: 9781.4590 - fp: 1929.0000 - tn: 274433.5938 - tp: 68.0000 - precision: 0.0341 - recall: 0.0069 - val_loss: 3.6982e-05 - val_fn: 9994.8457 - val_fp: 1929.0000 - val_tn: 280322.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0068\n",
      "Epoch 25/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 3.7457e-05 - fn: 10212.7793 - fp: 1929.0000 - tn: 286206.3125 - tp: 68.0000 - precision: 0.0341 - recall: 0.0066 - val_loss: 3.4023e-05 - val_fn: 10414.8457 - val_fp: 1929.0000 - val_tn: 292106.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0065\n",
      "Epoch 26/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 3.4807e-05 - fn: 10627.4824 - fp: 1929.0000 - tn: 297995.4688 - tp: 68.0000 - precision: 0.0341 - recall: 0.0064 - val_loss: 3.1516e-05 - val_fn: 10834.8457 - val_fp: 1929.0000 - val_tn: 303890.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0062\n",
      "Epoch 27/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 3.2581e-05 - fn: 11043.7471 - fp: 1929.0000 - tn: 309783.3125 - tp: 68.0000 - precision: 0.0341 - recall: 0.0061 - val_loss: 2.9396e-05 - val_fn: 11254.8457 - val_fp: 1929.0000 - val_tn: 315674.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0060\n",
      "Epoch 28/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 3.0706e-05 - fn: 11468.1543 - fp: 1929.0000 - tn: 321562.5000 - tp: 68.0000 - precision: 0.0341 - recall: 0.0059 - val_loss: 2.7614e-05 - val_fn: 11674.8457 - val_fp: 1929.0000 - val_tn: 327458.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0058\n",
      "Epoch 29/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.9137e-05 - fn: 11885.6426 - fp: 1929.0000 - tn: 333349.3125 - tp: 68.0000 - precision: 0.0341 - recall: 0.0057 - val_loss: 2.6112e-05 - val_fn: 12094.8457 - val_fp: 1929.0000 - val_tn: 339242.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0056\n",
      "Epoch 30/150\n",
      "10983/10983 [==============================] - 1s 69us/step - loss: 2.7830e-05 - fn: 12312.0205 - fp: 1929.0000 - tn: 345127.0938 - tp: 68.0000 - precision: 0.0341 - recall: 0.0055 - val_loss: 2.4855e-05 - val_fn: 12514.8457 - val_fp: 1929.0000 - val_tn: 351026.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0054\n",
      "Epoch 31/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 2.6747e-05 - fn: 12726.7822 - fp: 1929.0000 - tn: 356916.2812 - tp: 68.0000 - precision: 0.0341 - recall: 0.0053 - val_loss: 2.3807e-05 - val_fn: 12934.8457 - val_fp: 1929.0000 - val_tn: 362810.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0052\n",
      "Epoch 32/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.5858e-05 - fn: 13147.4971 - fp: 1929.0000 - tn: 368699.5625 - tp: 68.0000 - precision: 0.0341 - recall: 0.0051 - val_loss: 2.2944e-05 - val_fn: 13354.8457 - val_fp: 1929.0000 - val_tn: 374594.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0051\n",
      "Epoch 33/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.5134e-05 - fn: 13562.1133 - fp: 1929.0000 - tn: 380488.8125 - tp: 68.0000 - precision: 0.0341 - recall: 0.0050 - val_loss: 2.2233e-05 - val_fn: 13774.8457 - val_fp: 1929.0000 - val_tn: 386378.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0049\n",
      "Epoch 34/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.4547e-05 - fn: 13995.5811 - fp: 1929.0000 - tn: 392259.3438 - tp: 68.0000 - precision: 0.0341 - recall: 0.0048 - val_loss: 2.1653e-05 - val_fn: 14194.8457 - val_fp: 1929.0000 - val_tn: 398162.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0048\n",
      "Epoch 35/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.4079e-05 - fn: 14410.2529 - fp: 1929.0000 - tn: 404048.5625 - tp: 68.0000 - precision: 0.0341 - recall: 0.0047 - val_loss: 2.1180e-05 - val_fn: 14614.8457 - val_fp: 1929.0000 - val_tn: 409946.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0046\n",
      "Epoch 36/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.3711e-05 - fn: 14819.9102 - fp: 1929.0000 - tn: 415843.1250 - tp: 68.0000 - precision: 0.0341 - recall: 0.0046 - val_loss: 2.0806e-05 - val_fn: 15034.8457 - val_fp: 1929.0000 - val_tn: 421730.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0045\n",
      "Epoch 37/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.3427e-05 - fn: 15245.9561 - fp: 1929.0000 - tn: 427621.0625 - tp: 68.0000 - precision: 0.0341 - recall: 0.0044 - val_loss: 2.0513e-05 - val_fn: 15454.8457 - val_fp: 1929.0000 - val_tn: 433514.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0044\n",
      "Epoch 38/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.3211e-05 - fn: 15664.5928 - fp: 1929.0000 - tn: 439406.4062 - tp: 68.0000 - precision: 0.0341 - recall: 0.0043 - val_loss: 2.0280e-05 - val_fn: 15874.8457 - val_fp: 1929.0000 - val_tn: 445298.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0043\n",
      "Epoch 39/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.3052e-05 - fn: 16086.3984 - fp: 1929.0000 - tn: 451188.5938 - tp: 68.0000 - precision: 0.0341 - recall: 0.0042 - val_loss: 2.0107e-05 - val_fn: 16294.8457 - val_fp: 1929.0000 - val_tn: 457082.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0042\n",
      "Epoch 40/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 2.2935e-05 - fn: 16508.3457 - fp: 1929.0000 - tn: 462970.6562 - tp: 68.0000 - precision: 0.0341 - recall: 0.0041 - val_loss: 1.9972e-05 - val_fn: 16714.8457 - val_fp: 1929.0000 - val_tn: 468866.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0041\n",
      "Epoch 41/150\n",
      "10983/10983 [==============================] - 1s 63us/step - loss: 2.2853e-05 - fn: 16921.7578 - fp: 1929.0000 - tn: 474761.1562 - tp: 68.0000 - precision: 0.0341 - recall: 0.0040 - val_loss: 1.9874e-05 - val_fn: 17134.8457 - val_fp: 1929.0000 - val_tn: 480650.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0040\n",
      "Epoch 42/150\n",
      "10983/10983 [==============================] - 1s 69us/step - loss: 2.2798e-05 - fn: 17342.5781 - fp: 1929.0000 - tn: 486544.4062 - tp: 68.0000 - precision: 0.0341 - recall: 0.0039 - val_loss: 1.9801e-05 - val_fn: 17554.8457 - val_fp: 1929.0000 - val_tn: 492434.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0039\n",
      "Epoch 43/150\n",
      "10983/10983 [==============================] - 1s 63us/step - loss: 2.2762e-05 - fn: 17765.5840 - fp: 1929.0000 - tn: 498325.4375 - tp: 68.0000 - precision: 0.0341 - recall: 0.0038 - val_loss: 1.9748e-05 - val_fn: 17974.8457 - val_fp: 1929.0000 - val_tn: 504218.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0038\n",
      "Epoch 44/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2740e-05 - fn: 18177.9766 - fp: 1929.0000 - tn: 510116.9375 - tp: 68.0000 - precision: 0.0341 - recall: 0.0037 - val_loss: 1.9715e-05 - val_fn: 18394.8457 - val_fp: 1929.0000 - val_tn: 516002.4688 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0037\n",
      "Epoch 45/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 2.2725e-05 - fn: 18604.4941 - fp: 1929.0000 - tn: 521894.5625 - tp: 68.0000 - precision: 0.0341 - recall: 0.0036 - val_loss: 1.9689e-05 - val_fn: 18814.8457 - val_fp: 1929.0000 - val_tn: 527786.4375 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0036\n",
      "Epoch 46/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2717e-05 - fn: 19034.2207 - fp: 1929.0000 - tn: 533668.5000 - tp: 68.0000 - precision: 0.0341 - recall: 0.0036 - val_loss: 1.9671e-05 - val_fn: 19234.8457 - val_fp: 1929.0000 - val_tn: 539570.4375 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0035\n",
      "Epoch 47/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 2.2712e-05 - fn: 19442.3945 - fp: 1929.0000 - tn: 545464.6875 - tp: 68.0000 - precision: 0.0341 - recall: 0.0035 - val_loss: 1.9655e-05 - val_fn: 19654.8457 - val_fp: 1929.0000 - val_tn: 551354.3750 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0034\n",
      "Epoch 48/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2710e-05 - fn: 19864.7676 - fp: 1929.0000 - tn: 557246.0625 - tp: 68.0000 - precision: 0.0341 - recall: 0.0034 - val_loss: 1.9648e-05 - val_fn: 20074.8457 - val_fp: 1929.0000 - val_tn: 563138.3750 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0034\n",
      "Epoch 49/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 2.2708e-05 - fn: 20287.8438 - fp: 1929.0000 - tn: 569027.3750 - tp: 68.0000 - precision: 0.0341 - recall: 0.0033 - val_loss: 1.9643e-05 - val_fn: 20494.8457 - val_fp: 1929.0000 - val_tn: 574922.3750 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0033\n",
      "Epoch 50/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 20704.8145 - fp: 1929.0000 - tn: 580814.0625 - tp: 68.0000 - precision: 0.0341 - recall: 0.0033 - val_loss: 1.9637e-05 - val_fn: 20914.8457 - val_fp: 1929.0000 - val_tn: 586706.4375 - val_tp: 68.0000 - val_precision: 0.0341 - val_recall: 0.0032\n",
      "Epoch 51/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 2.3158e-05 - fn: 21133.2617 - fp: 1929.5901 - tn: 592589.0625 - tp: 68.0000 - precision: 0.0340 - recall: 0.0032 - val_loss: 1.9621e-05 - val_fn: 21334.8457 - val_fp: 1930.0000 - val_tn: 598489.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0032\n",
      "Epoch 52/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2696e-05 - fn: 21550.0293 - fp: 1930.0000 - tn: 604375.9375 - tp: 68.0000 - precision: 0.0340 - recall: 0.0031 - val_loss: 1.9621e-05 - val_fn: 21754.8457 - val_fp: 1930.0000 - val_tn: 610273.5000 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0031\n",
      "Epoch 53/150\n",
      "10983/10983 [==============================] - 1s 55us/step - loss: 2.2711e-05 - fn: 21968.0293 - fp: 1930.0000 - tn: 616162.1875 - tp: 68.0000 - precision: 0.0340 - recall: 0.0031 - val_loss: 1.9623e-05 - val_fn: 22174.8457 - val_fp: 1930.0000 - val_tn: 622057.5000 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0031\n",
      "Epoch 54/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2713e-05 - fn: 22390.8555 - fp: 1930.0000 - tn: 627943.1875 - tp: 68.0000 - precision: 0.0340 - recall: 0.0030 - val_loss: 1.9623e-05 - val_fn: 22594.8457 - val_fp: 1930.0000 - val_tn: 633841.5000 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0030\n",
      "Epoch 55/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2708e-05 - fn: 22807.2324 - fp: 1930.0000 - tn: 639730.6875 - tp: 68.0000 - precision: 0.0340 - recall: 0.0030 - val_loss: 1.9625e-05 - val_fn: 23014.8457 - val_fp: 1930.0000 - val_tn: 645625.5000 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0029\n",
      "Epoch 56/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2709e-05 - fn: 23228.7734 - fp: 1930.0000 - tn: 651513.3125 - tp: 68.0000 - precision: 0.0340 - recall: 0.0029 - val_loss: 1.9626e-05 - val_fn: 23434.8457 - val_fp: 1930.0000 - val_tn: 657409.5000 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0029\n",
      "Epoch 57/150\n",
      "10983/10983 [==============================] - 1s 68us/step - loss: 2.2706e-05 - fn: 23642.4746 - fp: 1930.0000 - tn: 663303.2500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0029 - val_loss: 1.9626e-05 - val_fn: 23854.8457 - val_fp: 1930.0000 - val_tn: 669193.5000 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0028\n",
      "Epoch 58/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 24062.5078 - fp: 1930.0000 - tn: 675087.1875 - tp: 68.0000 - precision: 0.0340 - recall: 0.0028 - val_loss: 1.9627e-05 - val_fn: 24274.8457 - val_fp: 1930.0000 - val_tn: 680977.5000 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0028\n",
      "Epoch 59/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2708e-05 - fn: 24489.5820 - fp: 1930.0000 - tn: 686864.5000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0028 - val_loss: 1.9626e-05 - val_fn: 24694.8457 - val_fp: 1930.0000 - val_tn: 692761.5000 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0027\n",
      "Epoch 60/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2706e-05 - fn: 24909.5371 - fp: 1930.0000 - tn: 698648.7500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0027 - val_loss: 1.9628e-05 - val_fn: 25114.8457 - val_fp: 1930.0000 - val_tn: 704545.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0027\n",
      "Epoch 61/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2708e-05 - fn: 25328.6445 - fp: 1930.0000 - tn: 710433.4375 - tp: 68.0000 - precision: 0.0340 - recall: 0.0027 - val_loss: 1.9626e-05 - val_fn: 25534.8457 - val_fp: 1930.0000 - val_tn: 716329.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0027\n",
      "Epoch 62/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.2707e-05 - fn: 25743.5195 - fp: 1930.0000 - tn: 722222.5000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0026 - val_loss: 1.9628e-05 - val_fn: 25954.8457 - val_fp: 1930.0000 - val_tn: 728113.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0026\n",
      "Epoch 63/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.2705e-05 - fn: 26165.1660 - fp: 1930.0000 - tn: 734005.0625 - tp: 68.0000 - precision: 0.0340 - recall: 0.0026 - val_loss: 1.9628e-05 - val_fn: 26374.8457 - val_fp: 1930.0000 - val_tn: 739897.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0026\n",
      "Epoch 64/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 2.2700e-05 - fn: 26582.9121 - fp: 1930.0000 - tn: 745790.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0026 - val_loss: 1.9626e-05 - val_fn: 26794.8457 - val_fp: 1930.0000 - val_tn: 751681.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0025\n",
      "Epoch 65/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 2.2711e-05 - fn: 27005.3164 - fp: 1930.0000 - tn: 757572.6875 - tp: 68.0000 - precision: 0.0340 - recall: 0.0025 - val_loss: 1.9627e-05 - val_fn: 27214.8457 - val_fp: 1930.0000 - val_tn: 763465.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0025\n",
      "Epoch 66/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2708e-05 - fn: 27420.9590 - fp: 1930.0000 - tn: 769360.7500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0025 - val_loss: 1.9626e-05 - val_fn: 27634.8457 - val_fp: 1930.0000 - val_tn: 775249.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0025\n",
      "Epoch 67/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 2.2707e-05 - fn: 27842.6680 - fp: 1930.0000 - tn: 781143.2500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0024 - val_loss: 1.9626e-05 - val_fn: 28054.8457 - val_fp: 1930.0000 - val_tn: 787033.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0024\n",
      "Epoch 68/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 2.2708e-05 - fn: 28265.7812 - fp: 1930.0000 - tn: 792923.8125 - tp: 68.0000 - precision: 0.0340 - recall: 0.0024 - val_loss: 1.9627e-05 - val_fn: 28474.8457 - val_fp: 1930.0000 - val_tn: 798817.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0024\n",
      "Epoch 69/150\n",
      "10983/10983 [==============================] - 1s 64us/step - loss: 2.2707e-05 - fn: 28688.1445 - fp: 1930.0000 - tn: 804705.9375 - tp: 68.0000 - precision: 0.0340 - recall: 0.0024 - val_loss: 1.9628e-05 - val_fn: 28894.8457 - val_fp: 1930.0000 - val_tn: 810601.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0023\n",
      "Epoch 70/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2708e-05 - fn: 29099.0898 - fp: 1930.0000 - tn: 816499.0000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0023 - val_loss: 1.9627e-05 - val_fn: 29314.8457 - val_fp: 1930.0000 - val_tn: 822385.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0023\n",
      "Epoch 71/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 2.2707e-05 - fn: 29518.4043 - fp: 1930.0000 - tn: 828283.1875 - tp: 68.0000 - precision: 0.0340 - recall: 0.0023 - val_loss: 1.9630e-05 - val_fn: 29734.8457 - val_fp: 1930.0000 - val_tn: 834169.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0023\n",
      "Epoch 72/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 29944.1855 - fp: 1930.0000 - tn: 840062.3125 - tp: 68.0000 - precision: 0.0340 - recall: 0.0023 - val_loss: 1.9630e-05 - val_fn: 30154.8457 - val_fp: 1930.0000 - val_tn: 845953.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0022\n",
      "Epoch 73/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 30369.3633 - fp: 1930.0000 - tn: 851840.9375 - tp: 68.0000 - precision: 0.0340 - recall: 0.0022 - val_loss: 1.9630e-05 - val_fn: 30574.8457 - val_fp: 1930.0000 - val_tn: 857737.5625 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0022\n",
      "Epoch 74/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 2.2707e-05 - fn: 30786.4883 - fp: 1930.0000 - tn: 863627.1875 - tp: 68.0000 - precision: 0.0340 - recall: 0.0022 - val_loss: 1.9628e-05 - val_fn: 30994.8457 - val_fp: 1930.0000 - val_tn: 869521.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0022\n",
      "Epoch 75/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.2707e-05 - fn: 31208.9277 - fp: 1930.0000 - tn: 875408.9375 - tp: 68.0000 - precision: 0.0340 - recall: 0.0022 - val_loss: 1.9629e-05 - val_fn: 31414.8457 - val_fp: 1930.0000 - val_tn: 881305.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0022\n",
      "Epoch 76/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 31628.6602 - fp: 1930.0000 - tn: 887193.3125 - tp: 68.0000 - precision: 0.0340 - recall: 0.0021 - val_loss: 1.9629e-05 - val_fn: 31834.8457 - val_fp: 1930.0000 - val_tn: 893089.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0021\n",
      "Epoch 77/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 32042.4863 - fp: 1930.0000 - tn: 898983.4375 - tp: 68.0000 - precision: 0.0340 - recall: 0.0021 - val_loss: 1.9629e-05 - val_fn: 32254.8457 - val_fp: 1930.0000 - val_tn: 904873.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0021\n",
      "Epoch 78/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 2.2707e-05 - fn: 32457.7305 - fp: 1930.0000 - tn: 910772.5625 - tp: 68.0000 - precision: 0.0340 - recall: 0.0021 - val_loss: 1.9629e-05 - val_fn: 32674.8457 - val_fp: 1930.0000 - val_tn: 916657.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0021\n",
      "Epoch 79/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 32887.8672 - fp: 1930.0000 - tn: 922545.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0021 - val_loss: 1.9628e-05 - val_fn: 33094.8477 - val_fp: 1930.0000 - val_tn: 928441.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0021\n",
      "Epoch 80/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 33313.2227 - fp: 1930.0000 - tn: 934324.9375 - tp: 68.0000 - precision: 0.0340 - recall: 0.0020 - val_loss: 1.9632e-05 - val_fn: 33514.8477 - val_fp: 1930.0000 - val_tn: 940225.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0020\n",
      "Epoch 81/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 33719.8594 - fp: 1930.0000 - tn: 946122.0625 - tp: 68.0000 - precision: 0.0340 - recall: 0.0020 - val_loss: 1.9630e-05 - val_fn: 33934.8477 - val_fp: 1930.0000 - val_tn: 952009.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0020\n",
      "Epoch 82/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 34141.1367 - fp: 1930.0000 - tn: 957904.6250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0020 - val_loss: 1.9629e-05 - val_fn: 34354.8477 - val_fp: 1930.0000 - val_tn: 963793.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0020\n",
      "Epoch 83/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 34567.8867 - fp: 1930.0000 - tn: 969681.9375 - tp: 68.0000 - precision: 0.0340 - recall: 0.0020 - val_loss: 1.9630e-05 - val_fn: 34774.8477 - val_fp: 1930.0000 - val_tn: 975577.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0020\n",
      "Epoch 84/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 2.2707e-05 - fn: 34982.5508 - fp: 1930.0000 - tn: 981471.4375 - tp: 68.0000 - precision: 0.0340 - recall: 0.0019 - val_loss: 1.9629e-05 - val_fn: 35194.8477 - val_fp: 1930.0000 - val_tn: 987361.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0019\n",
      "Epoch 85/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 2.2707e-05 - fn: 35405.5430 - fp: 1930.0000 - tn: 993252.9375 - tp: 68.0000 - precision: 0.0340 - recall: 0.0019 - val_loss: 1.9629e-05 - val_fn: 35614.8477 - val_fp: 1930.0000 - val_tn: 999145.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0019\n",
      "Epoch 86/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 35827.8008 - fp: 1930.0000 - tn: 1005033.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0019 - val_loss: 1.9628e-05 - val_fn: 36034.8477 - val_fp: 1930.0000 - val_tn: 1010929.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0019\n",
      "Epoch 87/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.2707e-05 - fn: 36241.2969 - fp: 1930.0000 - tn: 1016824.1875 - tp: 68.0000 - precision: 0.0340 - recall: 0.0019 - val_loss: 1.9629e-05 - val_fn: 36454.8477 - val_fp: 1930.0000 - val_tn: 1022713.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0019\n",
      "Epoch 88/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 2.2707e-05 - fn: 36666.7188 - fp: 1930.0000 - tn: 1028603.2500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0019 - val_loss: 1.9629e-05 - val_fn: 36874.8477 - val_fp: 1930.0000 - val_tn: 1034497.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0018\n",
      "Epoch 89/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 37083.9102 - fp: 1930.0000 - tn: 1040389.7500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0018 - val_loss: 1.9629e-05 - val_fn: 37294.8477 - val_fp: 1930.0000 - val_tn: 1046281.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0018\n",
      "Epoch 90/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2708e-05 - fn: 37503.2539 - fp: 1930.0000 - tn: 1052175.1250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0018 - val_loss: 1.9630e-05 - val_fn: 37714.8477 - val_fp: 1930.0000 - val_tn: 1058065.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0018\n",
      "Epoch 91/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 37921.7461 - fp: 1930.0000 - tn: 1063960.0000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0018 - val_loss: 1.9629e-05 - val_fn: 38134.8477 - val_fp: 1930.0000 - val_tn: 1069849.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0018\n",
      "Epoch 92/150\n",
      "10983/10983 [==============================] - 1s 67us/step - loss: 2.2707e-05 - fn: 38340.4805 - fp: 1930.0000 - tn: 1075745.2500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0018 - val_loss: 1.9629e-05 - val_fn: 38554.8477 - val_fp: 1930.0000 - val_tn: 1081633.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0018\n",
      "Epoch 93/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.2707e-05 - fn: 38767.4570 - fp: 1930.0000 - tn: 1087522.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0018 - val_loss: 1.9629e-05 - val_fn: 38974.8477 - val_fp: 1930.0000 - val_tn: 1093417.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0017\n",
      "Epoch 94/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 2.2707e-05 - fn: 39194.9062 - fp: 1930.0000 - tn: 1099298.5000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0017 - val_loss: 1.9628e-05 - val_fn: 39394.8477 - val_fp: 1930.0000 - val_tn: 1105201.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0017\n",
      "Epoch 95/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 2.2707e-05 - fn: 39605.1836 - fp: 1930.0000 - tn: 1111093.1250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0017 - val_loss: 1.9628e-05 - val_fn: 39814.8477 - val_fp: 1930.0000 - val_tn: 1116985.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0017\n",
      "Epoch 96/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 2.2707e-05 - fn: 40036.6992 - fp: 1930.0000 - tn: 1122865.0000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0017 - val_loss: 1.9627e-05 - val_fn: 40234.8477 - val_fp: 1930.0000 - val_tn: 1128769.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0017\n",
      "Epoch 97/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 40440.4531 - fp: 1930.0000 - tn: 1134665.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0017 - val_loss: 1.9627e-05 - val_fn: 40654.8477 - val_fp: 1930.0000 - val_tn: 1140553.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0017\n",
      "Epoch 98/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 40868.1836 - fp: 1930.0000 - tn: 1146441.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0017 - val_loss: 1.9628e-05 - val_fn: 41074.8477 - val_fp: 1930.0000 - val_tn: 1152337.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0017\n",
      "Epoch 99/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 41300.4141 - fp: 1930.0000 - tn: 1158212.5000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0016 - val_loss: 1.9625e-05 - val_fn: 41494.8477 - val_fp: 1930.0000 - val_tn: 1164121.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0016\n",
      "Epoch 100/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 41710.7891 - fp: 1930.0000 - tn: 1170007.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0016 - val_loss: 1.9625e-05 - val_fn: 41914.8477 - val_fp: 1930.0000 - val_tn: 1175905.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0016\n",
      "Epoch 101/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 42120.0273 - fp: 1930.0000 - tn: 1181802.6250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0016 - val_loss: 1.9628e-05 - val_fn: 42334.8477 - val_fp: 1930.0000 - val_tn: 1187689.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0016\n",
      "Epoch 102/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 42543.8047 - fp: 1930.0000 - tn: 1193582.1250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0016 - val_loss: 1.9631e-05 - val_fn: 42754.8477 - val_fp: 1930.0000 - val_tn: 1199473.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0016\n",
      "Epoch 103/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 42962.8359 - fp: 1930.0000 - tn: 1205367.2500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0016 - val_loss: 1.9629e-05 - val_fn: 43174.8477 - val_fp: 1930.0000 - val_tn: 1211257.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0016\n",
      "Epoch 104/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 2.2707e-05 - fn: 43381.6406 - fp: 1930.0000 - tn: 1217152.6250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0016 - val_loss: 1.9628e-05 - val_fn: 43594.8477 - val_fp: 1930.0000 - val_tn: 1223041.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0016\n",
      "Epoch 105/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 43802.0781 - fp: 1930.0000 - tn: 1228936.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0016 - val_loss: 1.9628e-05 - val_fn: 44014.8477 - val_fp: 1930.0000 - val_tn: 1234825.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0015\n",
      "Epoch 106/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 2.2707e-05 - fn: 44220.9844 - fp: 1930.0000 - tn: 1240720.7500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0015 - val_loss: 1.9629e-05 - val_fn: 44434.8477 - val_fp: 1930.0000 - val_tn: 1246609.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0015\n",
      "Epoch 107/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 2.2707e-05 - fn: 44649.8984 - fp: 1930.0000 - tn: 1252495.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0015 - val_loss: 1.9628e-05 - val_fn: 44854.8477 - val_fp: 1930.0000 - val_tn: 1258393.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0015\n",
      "Epoch 108/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 45067.6133 - fp: 1930.0000 - tn: 1264281.7500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0015 - val_loss: 1.9629e-05 - val_fn: 45274.8477 - val_fp: 1930.0000 - val_tn: 1270177.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0015\n",
      "Epoch 109/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 45477.6211 - fp: 1930.0000 - tn: 1276076.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0015 - val_loss: 1.9631e-05 - val_fn: 45694.8477 - val_fp: 1930.0000 - val_tn: 1281961.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0015\n",
      "Epoch 110/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 2.2707e-05 - fn: 45896.7500 - fp: 1930.0000 - tn: 1287861.5000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0015 - val_loss: 1.9631e-05 - val_fn: 46114.8477 - val_fp: 1930.0000 - val_tn: 1293745.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0015\n",
      "Epoch 111/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 46324.6797 - fp: 1930.0000 - tn: 1299637.2500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0015 - val_loss: 1.9629e-05 - val_fn: 46534.8477 - val_fp: 1930.0000 - val_tn: 1305529.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0015\n",
      "Epoch 112/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 2.2708e-05 - fn: 46743.5156 - fp: 1930.0000 - tn: 1311422.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0015 - val_loss: 1.9631e-05 - val_fn: 46954.8477 - val_fp: 1930.0000 - val_tn: 1317313.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0014\n",
      "Epoch 113/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.2707e-05 - fn: 47158.6992 - fp: 1930.0000 - tn: 1323211.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0014 - val_loss: 1.9632e-05 - val_fn: 47374.8477 - val_fp: 1930.0000 - val_tn: 1329097.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0014\n",
      "Epoch 114/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 47595.7500 - fp: 1930.0000 - tn: 1334977.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0014 - val_loss: 1.9629e-05 - val_fn: 47794.8477 - val_fp: 1930.0000 - val_tn: 1340881.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0014\n",
      "Epoch 115/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.2707e-05 - fn: 48005.3008 - fp: 1930.0000 - tn: 1346772.2500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0014 - val_loss: 1.9629e-05 - val_fn: 48214.8477 - val_fp: 1930.0000 - val_tn: 1352665.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0014\n",
      "Epoch 116/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 48424.8477 - fp: 1930.0000 - tn: 1358556.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0014 - val_loss: 1.9630e-05 - val_fn: 48634.8477 - val_fp: 1930.0000 - val_tn: 1364449.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0014\n",
      "Epoch 117/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 48843.0820 - fp: 1930.0000 - tn: 1370343.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0014 - val_loss: 1.9630e-05 - val_fn: 49054.8477 - val_fp: 1930.0000 - val_tn: 1376233.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0014\n",
      "Epoch 118/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 49276.5820 - fp: 1930.0000 - tn: 1382113.2500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0014 - val_loss: 1.9627e-05 - val_fn: 49474.8477 - val_fp: 1930.0000 - val_tn: 1388017.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0014\n",
      "Epoch 119/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 49678.0117 - fp: 1930.0000 - tn: 1393915.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0014 - val_loss: 1.9628e-05 - val_fn: 49894.8477 - val_fp: 1930.0000 - val_tn: 1399801.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0014\n",
      "Epoch 120/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 50105.6055 - fp: 1930.0000 - tn: 1405693.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0014 - val_loss: 1.9629e-05 - val_fn: 50314.8477 - val_fp: 1930.0000 - val_tn: 1411585.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0013\n",
      "Epoch 121/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 50527.5703 - fp: 1930.0000 - tn: 1417474.7500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0013 - val_loss: 1.9627e-05 - val_fn: 50734.8477 - val_fp: 1930.0000 - val_tn: 1423369.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0013\n",
      "Epoch 122/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 50952.5469 - fp: 1930.0000 - tn: 1429253.5000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0013 - val_loss: 1.9628e-05 - val_fn: 51154.8477 - val_fp: 1930.0000 - val_tn: 1435153.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0013\n",
      "Epoch 123/150\n",
      "10983/10983 [==============================] - 1s 63us/step - loss: 2.2707e-05 - fn: 51362.9414 - fp: 1930.0000 - tn: 1441046.6250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0013 - val_loss: 1.9627e-05 - val_fn: 51574.8477 - val_fp: 1930.0000 - val_tn: 1446937.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0013\n",
      "Epoch 124/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.2707e-05 - fn: 51783.4414 - fp: 1930.0000 - tn: 1452830.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0013 - val_loss: 1.9629e-05 - val_fn: 51994.8477 - val_fp: 1930.0000 - val_tn: 1458721.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0013\n",
      "Epoch 125/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 2.2707e-05 - fn: 52204.1055 - fp: 1930.0000 - tn: 1464614.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0013 - val_loss: 1.9628e-05 - val_fn: 52414.8477 - val_fp: 1930.0000 - val_tn: 1470505.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0013\n",
      "Epoch 126/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 52618.6094 - fp: 1930.0000 - tn: 1476403.7500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0013 - val_loss: 1.9631e-05 - val_fn: 52834.8477 - val_fp: 1930.0000 - val_tn: 1482289.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0013\n",
      "Epoch 127/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 53034.3438 - fp: 1930.0000 - tn: 1488191.6250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0013 - val_loss: 1.9631e-05 - val_fn: 53254.8477 - val_fp: 1930.0000 - val_tn: 1494073.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0013\n",
      "Epoch 128/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2708e-05 - fn: 53456.1172 - fp: 1930.0000 - tn: 1499974.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0013 - val_loss: 1.9629e-05 - val_fn: 53674.8477 - val_fp: 1930.0000 - val_tn: 1505857.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0013\n",
      "Epoch 129/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 53874.1875 - fp: 1930.0000 - tn: 1511759.7500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0013 - val_loss: 1.9633e-05 - val_fn: 54094.8477 - val_fp: 1930.0000 - val_tn: 1517641.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0013\n",
      "Epoch 130/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2708e-05 - fn: 54295.2773 - fp: 1930.0000 - tn: 1523542.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0013 - val_loss: 1.9630e-05 - val_fn: 54514.8477 - val_fp: 1930.0000 - val_tn: 1529425.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 131/150\n",
      "10983/10983 [==============================] - 1s 63us/step - loss: 2.2707e-05 - fn: 54722.7109 - fp: 1930.0000 - tn: 1535318.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9634e-05 - val_fn: 54934.8477 - val_fp: 1930.0000 - val_tn: 1541209.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 132/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.2707e-05 - fn: 55134.2539 - fp: 1930.0000 - tn: 1547112.0000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9633e-05 - val_fn: 55354.8477 - val_fp: 1930.0000 - val_tn: 1552993.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 133/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 55562.3438 - fp: 1930.0000 - tn: 1558888.1250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9632e-05 - val_fn: 55774.8477 - val_fp: 1930.0000 - val_tn: 1564777.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 134/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 55992.0859 - fp: 1930.0000 - tn: 1570661.6250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9631e-05 - val_fn: 56194.8477 - val_fp: 1930.0000 - val_tn: 1576561.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 135/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 56402.4648 - fp: 1930.0000 - tn: 1582456.0000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9629e-05 - val_fn: 56614.8477 - val_fp: 1930.0000 - val_tn: 1588345.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 136/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 56826.4648 - fp: 1930.0000 - tn: 1594235.1250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9629e-05 - val_fn: 57034.8477 - val_fp: 1930.0000 - val_tn: 1600129.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 137/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 57241.6523 - fp: 1930.0000 - tn: 1606023.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9627e-05 - val_fn: 57454.8477 - val_fp: 1930.0000 - val_tn: 1611913.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 138/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 57665.6758 - fp: 1930.0000 - tn: 1617804.6250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9628e-05 - val_fn: 57874.8477 - val_fp: 1930.0000 - val_tn: 1623697.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 139/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 58084.1055 - fp: 1930.0000 - tn: 1629590.5000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9631e-05 - val_fn: 58294.8477 - val_fp: 1930.0000 - val_tn: 1635481.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 140/150\n",
      "10983/10983 [==============================] - 1s 65us/step - loss: 2.2707e-05 - fn: 58502.3008 - fp: 1930.0000 - tn: 1641375.6250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9631e-05 - val_fn: 58714.8477 - val_fp: 1930.0000 - val_tn: 1647265.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0012\n",
      "Epoch 141/150\n",
      "10983/10983 [==============================] - 1s 61us/step - loss: 2.2707e-05 - fn: 58927.1055 - fp: 1930.0000 - tn: 1653154.7500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0012 - val_loss: 1.9629e-05 - val_fn: 59134.8477 - val_fp: 1930.0000 - val_tn: 1659049.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0011\n",
      "Epoch 142/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 59359.9766 - fp: 1930.0000 - tn: 1664926.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0011 - val_loss: 1.9629e-05 - val_fn: 59554.8477 - val_fp: 1930.0000 - val_tn: 1670833.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0011\n",
      "Epoch 143/150\n",
      "10983/10983 [==============================] - 1s 59us/step - loss: 2.2707e-05 - fn: 59772.3945 - fp: 1930.0000 - tn: 1676717.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0011 - val_loss: 1.9627e-05 - val_fn: 59974.8477 - val_fp: 1930.0000 - val_tn: 1682617.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0011\n",
      "Epoch 144/150\n",
      "10983/10983 [==============================] - 1s 60us/step - loss: 2.2707e-05 - fn: 60181.9883 - fp: 1930.0000 - tn: 1688512.1250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0011 - val_loss: 1.9627e-05 - val_fn: 60394.8477 - val_fp: 1930.0000 - val_tn: 1694401.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0011\n",
      "Epoch 145/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 60608.0352 - fp: 1930.0000 - tn: 1700290.0000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0011 - val_loss: 1.9628e-05 - val_fn: 60814.8477 - val_fp: 1930.0000 - val_tn: 1706185.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0011\n",
      "Epoch 146/150\n",
      "10983/10983 [==============================] - 1s 57us/step - loss: 2.2707e-05 - fn: 61025.1211 - fp: 1930.0000 - tn: 1712076.8750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0011 - val_loss: 1.9627e-05 - val_fn: 61234.8477 - val_fp: 1930.0000 - val_tn: 1717969.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0011\n",
      "Epoch 147/150\n",
      "10983/10983 [==============================] - 1s 62us/step - loss: 2.2707e-05 - fn: 61447.7852 - fp: 1930.0000 - tn: 1723858.0000 - tp: 68.0000 - precision: 0.0340 - recall: 0.0011 - val_loss: 1.9627e-05 - val_fn: 61654.8477 - val_fp: 1930.0000 - val_tn: 1729753.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0011\n",
      "Epoch 148/150\n",
      "10983/10983 [==============================] - 1s 56us/step - loss: 2.2707e-05 - fn: 61867.5234 - fp: 1930.0000 - tn: 1735642.7500 - tp: 68.0000 - precision: 0.0340 - recall: 0.0011 - val_loss: 1.9627e-05 - val_fn: 62074.8477 - val_fp: 1930.0000 - val_tn: 1741537.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0011\n",
      "Epoch 149/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 62286.4180 - fp: 1930.0000 - tn: 1747427.3750 - tp: 68.0000 - precision: 0.0340 - recall: 0.0011 - val_loss: 1.9627e-05 - val_fn: 62494.8477 - val_fp: 1930.0000 - val_tn: 1753321.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0011\n",
      "Epoch 150/150\n",
      "10983/10983 [==============================] - 1s 58us/step - loss: 2.2707e-05 - fn: 62712.8008 - fp: 1930.0000 - tn: 1759204.6250 - tp: 68.0000 - precision: 0.0340 - recall: 0.0011 - val_loss: 1.9627e-05 - val_fn: 62914.8477 - val_fp: 1930.0000 - val_tn: 1765105.6250 - val_tp: 68.0000 - val_precision: 0.0340 - val_recall: 0.0011\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f9c9d6aca90>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "counts = np.bincount(y_train_numpy)\n",
    "\n",
    "all_nba_weight = 1.0/counts[0]\n",
    "non_all_nba_weight = 1.0/counts[1]\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.FalseNegatives(name='fn'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "]\n",
    "\n",
    "neural_model.compile(\n",
    "    optimizer = keras.optimizers.Adam(),loss=\"binary_crossentropy\",metrics=metrics\n",
    ")\n",
    "\n",
    "class_weight = {0: non_all_nba_weight, 1: all_nba_weight}\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10,restore_best_weights = True)]\n",
    "\n",
    "neural_model.fit(\n",
    "    x_train_numpy,\n",
    "    y_train_numpy,\n",
    "    epochs=150,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    "    # callbacks = callbacks,\n",
    "    class_weight = class_weight\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2085/2085 [==============================] - 0s 24us/step\n",
      "[0.1928380664852861, 62964.15234375, 1930.0, 1766709.375, 68.0, 0.03403402119874954, 0.001078814617358148]\n",
      "0.9712230215827338\n",
      "\n",
      "      Unnamed: 0         Player Pos  Age   Tm   G    MP  FG_per_game  \\\n",
      "0              0   lex Abrines  SG   23  OKC  68  15.5          2.0   \n",
      "1              1     Quincy Acy  PF   26  TOT  38  14.7          1.8   \n",
      "2              2   Steven Adams   C   23  OKC  80  29.9          4.7   \n",
      "3              3  Arron Afflalo  SG   31  SAC  61  25.9          3.0   \n",
      "4              4  Alexis Ajina   C   28  NOP  39  15.0          2.3   \n",
      "...          ...            ...  ..  ...  ...  ..   ...          ...   \n",
      "2080        2080     Trae Young  PG   21  ATL  60  35.3          9.1   \n",
      "2081        2081    Cody Zeller   C   27  CHO  58  23.1          4.3   \n",
      "2082        2082   Tyler Zeller   C   30  SAS   2   2.0          0.5   \n",
      "2083        2083     Ante ii   C   23  CLE  22  10.0          1.9   \n",
      "2084        2084    Ivica Zubac   C   22  LAC  72  18.4          3.3   \n",
      "\n",
      "      FGA_per_game  FG%_per_game  ...  DWS_advanced  WS_advanced  \\\n",
      "0              5.0         0.393  ...           0.9          2.1   \n",
      "1              4.5         0.412  ...           0.5          0.9   \n",
      "2              8.2         0.571  ...           3.1          6.5   \n",
      "3              6.9         0.440  ...           0.2          1.4   \n",
      "4              4.6         0.500  ...           0.9          1.0   \n",
      "...            ...           ...  ...           ...          ...   \n",
      "2080          20.8         0.437  ...           0.6          5.9   \n",
      "2081           8.3         0.524  ...           1.3          3.6   \n",
      "2082           2.0         0.250  ...           0.0          0.0   \n",
      "2083           3.3         0.569  ...           0.2          0.5   \n",
      "2084           5.3         0.613  ...           2.3          6.6   \n",
      "\n",
      "      WS/48_advanced  OBPM_advanced  DBPM_advanced  BPM_advanced  \\\n",
      "0              0.096           -1.3           -0.4          -1.6   \n",
      "1              0.082           -1.5           -0.6          -2.1   \n",
      "2              0.130           -0.2            0.0          -0.2   \n",
      "3              0.043           -2.1           -1.5          -3.6   \n",
      "4              0.080           -4.0            0.7          -3.3   \n",
      "...              ...            ...            ...           ...   \n",
      "2080           0.133            6.2           -2.3           3.9   \n",
      "2081           0.129            0.2           -0.8          -0.6   \n",
      "2082          -0.075           -0.3          -22.1         -22.4   \n",
      "2083           0.106           -1.7           -1.5          -3.2   \n",
      "2084           0.241            1.9            0.8           2.8   \n",
      "\n",
      "      VORP_advanced  Year  All_NBA  Predicted_All_NBA  \n",
      "0               0.1  2017    False              False  \n",
      "1               0.0  2017    False              False  \n",
      "2               1.1  2017    False              False  \n",
      "3              -0.7  2017    False              False  \n",
      "4              -0.2  2017    False              False  \n",
      "...             ...   ...      ...                ...  \n",
      "2080            3.1  2020    False              False  \n",
      "2081            0.5  2020    False              False  \n",
      "2082            0.0  2020    False              False  \n",
      "2083           -0.1  2020    False              False  \n",
      "2084            1.6  2020    False              False  \n",
      "\n",
      "[2025 rows x 96 columns]\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Player, Pos, Age, Tm, G, MP, FG_per_game, FGA_per_game, FG%_per_game, 3P_per_game, 3PA_per_game, 3P%_per_game, 2P_per_game, 2PA_per_game, 2P%_per_game, eFG%_per_game, FT_per_game, FTA_per_game, FT%_per_game, ORB_per_game, DRB_per_game, TRB_per_game, AST_per_game, STL_per_game, BLK_per_game, TOV_per_game, PF_per_game, PTS_per_game, FG_per_minute, FGA_per_minute, FG%_per_minute, 3P_per_minute, 3PA_per_minute, 3P%_per_minute, 2P_per_minute, 2PA_per_minute, 2P%_per_minute, FT_per_minute, FTA_per_minute, FT%_per_minute, ORB_per_minute, DRB_per_minute, TRB_per_minute, AST_per_minute, STL_per_minute, BLK_per_minute, TOV_per_minute, PF_per_minute, PTS_per_minute, FG_per_poss, FGA_per_poss, FG%_per_poss, 3P_per_poss, 3PA_per_poss, 3P%_per_poss, 2P_per_poss, 2PA_per_poss, 2P%_per_poss, FT_per_poss, FTA_per_poss, FT%_per_poss, ORB_per_poss, DRB_per_poss, TRB_per_poss, AST_per_poss, STL_per_poss, BLK_per_poss, TOV_per_poss, PF_per_poss, PTS_per_poss, ORtg_per_poss, DRtg_per_poss, PER_advanced, TS%_advanced, 3PAr_advanced, FTr_advanced, ORB%_advanced, DRB%_advanced, TRB%_advanced, AST%_advanced, STL%_advanced, BLK%_advanced, TOV%_advanced, USG%_advanced, OWS_advanced, DWS_advanced, WS_advanced, WS/48_advanced, OBPM_advanced, DBPM_advanced, BPM_advanced, VORP_advanced, Year, All_NBA, Predicted_All_NBA]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 96 columns]\n",
      "\n",
      "Series([], Name: Player, dtype: object)\n",
      "\n",
      "15      Giannis Antetokounmpo\n",
      "65               Jimmy Butler\n",
      "97              Stephen Curry\n",
      "99              Anthony Davis\n",
      "107             DeMar DeRozan\n",
      "118              Kevin Durant\n",
      "154               Rudy Gobert\n",
      "163            Draymond Green\n",
      "171              James Harden\n",
      "219              LeBron James\n",
      "239            DeAndre Jordan\n",
      "260             Kawhi Leonard\n",
      "424             Isaiah Thomas\n",
      "452                 John Wall\n",
      "457         Russell Westbrook\n",
      "492         LaMarcus Aldridge\n",
      "501     Giannis Antetokounmpo\n",
      "564              Jimmy Butler\n",
      "605             Stephen Curry\n",
      "607             Anthony Davis\n",
      "615             DeMar DeRozan\n",
      "628              Kevin Durant\n",
      "632               Joel Embiid\n",
      "658               Paul George\n",
      "679              James Harden\n",
      "733              LeBron James\n",
      "783            Damian Lillard\n",
      "867            Victor Oladipo\n",
      "969        Karl-Anthony Towns\n",
      "993         Russell Westbrook\n",
      "1043    Giannis Antetokounmpo\n",
      "1149            Stephen Curry\n",
      "1175             Kevin Durant\n",
      "1180              Joel Embiid\n",
      "1208              Paul George\n",
      "1212              Rudy Gobert\n",
      "1227            Blake Griffin\n",
      "1231             James Harden\n",
      "1274             Kyrie Irving\n",
      "1283             LeBron James\n",
      "1294             Nikola Joki\n",
      "1326            Kawhi Leonard\n",
      "1330           Damian Lillard\n",
      "1522             Kemba Walker\n",
      "1531        Russell Westbrook\n",
      "1568    Giannis Antetokounmpo\n",
      "1637             Jimmy Butler\n",
      "1678            Anthony Davis\n",
      "1689              Luka Doni\n",
      "1731              Rudy Gobert\n",
      "1753             James Harden\n",
      "1805             LeBron James\n",
      "1816             Nikola Joki\n",
      "1849            Kawhi Leonard\n",
      "1852           Damian Lillard\n",
      "1951               Chris Paul\n",
      "2004            Pascal Siakam\n",
      "2006              Ben Simmons\n",
      "2019             Jayson Tatum\n",
      "2061        Russell Westbrook\n",
      "Name: Player, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predicting classes\n",
    "predicted_y = neural_model.predict(x_test_numpy)\n",
    "predicted_bool = convert(predicted_y)\n",
    "score = neural_model.evaluate(x_test_numpy,y_test_numpy)\n",
    "print(score)\n",
    "print(accuracy_score(y_test_numpy,predicted_bool))\n",
    "\n",
    "x_test_copy = test_data.copy()\n",
    "x_test_copy[\"Predicted_All_NBA\"] = predicted_bool\n",
    "print()\n",
    "\n",
    "print(x_test_copy.loc[x_test_copy.All_NBA == False])\n",
    "print()\n",
    "print(x_test_copy.loc[x_test_copy.Predicted_All_NBA == True])\n",
    "print()\n",
    "#players predicted to be on All-NBA and were not on All-NBA\n",
    "print(x_test_copy.loc[(x_test_copy.All_NBA == False) & (x_test_copy.Predicted_All_NBA == True),\"Player\"])\n",
    "print()\n",
    "#players predicted not to be on All-NBA and was listed on All-NBA\n",
    "print(x_test_copy.loc[(x_test_copy.All_NBA == True) & (x_test_copy.Predicted_All_NBA == False),\"Player\"])\n",
    "print()\n"
   ]
  }
 ]
}